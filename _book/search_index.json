[
["tests-for-experiments.html", "Chapter 7 Tests for experiments", " Chapter 7 Tests for experiments Prior to performing experiments, we need to set the dependent variables (outcome), and independent variables (explanatory variables). Other experimental components to consider include randomization, replication, blocking # load dependencies library(ggplot2) library(broom) library(tidyverse) library(pwr) library(haven) library(simputation) library(sampling) library(agricolae) library(naniar) library(DescTools) library(mice) load data: Dataset is on the Effect of Vitamin C on Tooth Growth in Guinea Pigs: data(ToothGrowth) ToothGrowth %&gt;% head(2) ## len supp dose ## 1 4.2 VC 0.5 ## 2 11.5 VC 0.5 Perform a two-sided t-test: t.test(x = ToothGrowth$len, alternative = &quot;two.sided&quot;, mu = 18) ## ## One Sample t-test ## ## data: ToothGrowth$len ## t = 0.82361, df = 59, p-value = 0.4135 ## alternative hypothesis: true mean is not equal to 18 ## 95 percent confidence interval: ## 16.83731 20.78936 ## sample estimates: ## mean of x ## 18.81333 Perform a t-test ToothGrowth_ttest &lt;- t.test(len ~ supp, data = ToothGrowth) ToothGrowth_ttest ## ## Welch Two Sample t-test ## ## data: len by supp ## t = 1.9153, df = 55.309, p-value = 0.06063 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.1710156 7.5710156 ## sample estimates: ## mean in group OJ mean in group VC ## 20.66333 16.96333 Tidy ToothGrowth_ttest: tidy(ToothGrowth_ttest) ## # A tibble: 1 x 10 ## estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3.70 20.7 17.0 1.92 0.0606 55.3 -0.171 7.57 ## # … with 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt; Replication: Count number of observations for each combination of supp and dose ToothGrowth %&gt;% count(supp, dose) ## supp dose n ## 1 OJ 0.5 10 ## 2 OJ 1.0 10 ## 3 OJ 2.0 10 ## 4 VC 0.5 10 ## 5 VC 1.0 10 ## 6 VC 2.0 10 Blocking: Create a boxplot with geom_boxplot() aov() creates a linear regression model by calling lm() and examining results with anova() all in one function call. ggplot(ToothGrowth, aes(x = dose, y = len)) + geom_boxplot() ## Warning: Continuous x aesthetic -- did you forget aes(group=...)? Create ToothGrowth_aov and Examine ToothGrowth_aov with summary(): ToothGrowth_aov &lt;- aov(len ~ dose + supp, data = ToothGrowth) summary(ToothGrowth_aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## dose 1 2224.3 2224.3 123.99 6.31e-16 *** ## supp 1 205.3 205.3 11.45 0.0013 ** ## Residuals 57 1022.6 17.9 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Hypothesis Testing (null and alternative) with pwr package one sided and two sided tests: - type ?t.test to find out more #Less than t.test(x = ToothGrowth$len, alternative = &quot;less&quot;, mu = 18) ## ## One Sample t-test ## ## data: ToothGrowth$len ## t = 0.82361, df = 59, p-value = 0.7933 ## alternative hypothesis: true mean is less than 18 ## 95 percent confidence interval: ## -Inf 20.46358 ## sample estimates: ## mean of x ## 18.81333 # Greater than t.test(x = ToothGrowth$len, alternative = &quot;greater&quot;, mu = 18) ## ## One Sample t-test ## ## data: ToothGrowth$len ## t = 0.82361, df = 59, p-value = 0.2067 ## alternative hypothesis: true mean is greater than 18 ## 95 percent confidence interval: ## 17.16309 Inf ## sample estimates: ## mean of x ## 18.81333 It turns out the mean of len is actually very close to 18, so neither of these tests tells us much about the mean of tooth length. ?pwr.t.test() Calculate sample size: pwr.t.test(n = NULL, d = 0.25, # small effect size of 0.25 sig.level = 0.05, type = &quot;one.sample&quot;, alternative = &quot;greater&quot;, power = 0.8) ## ## One-sample t test power calculation ## ## n = 100.2877 ## d = 0.25 ## sig.level = 0.05 ## power = 0.8 ## alternative = greater Calculate power: pwr.t.test(n = 100, d = 0.35, sig.level = 0.1, type = &quot;two.sample&quot;, alternative = &quot;two.sided&quot;, power = NULL) ## ## Two-sample t test power calculation ## ## n = 100 ## d = 0.35 ## sig.level = 0.1 ## power = 0.7943532 ## alternative = two.sided ## ## NOTE: n is number in *each* group power for multiple groups: pwr.anova.test(k = 3, n = 20, f = 0.2, #effect size sig.level = 0.05, power = NULL) ## ## Balanced one-way analysis of variance power calculation ## ## k = 3 ## n = 20 ## f = 0.2 ## sig.level = 0.05 ## power = 0.2521043 ## ## NOTE: n is number in each group Anova tests (for multiple groups) can be done in two ways Basic Experiments for exploratory data analysis including A/B testing get data: data(txhousing) txhousing %&gt;% head(2) ## # A tibble: 2 x 9 ## city year month sales volume median listings inventory date ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Abilene 2000 1 72 5380000 71400 701 6.3 2000 ## 2 Abilene 2000 2 98 6505000 58700 746 6.6 2000. remove NAs: tx_housing &lt;- na.omit(txhousing) # Examine the variables with glimpse() glimpse(tx_housing) ## Rows: 7,126 ## Columns: 9 ## $ city &lt;chr&gt; &quot;Abilene&quot;, &quot;Abilene&quot;, &quot;Abilene&quot;, &quot;Abilene&quot;, &quot;Abilene&quot;, &quot;Abi… ## $ year &lt;int&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000,… ## $ month &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7,… ## $ sales &lt;dbl&gt; 72, 98, 130, 98, 141, 156, 152, 131, 104, 101, 100, 92, 75,… ## $ volume &lt;dbl&gt; 5380000, 6505000, 9285000, 9730000, 10590000, 13910000, 126… ## $ median &lt;dbl&gt; 71400, 58700, 58100, 68600, 67300, 66900, 73500, 75000, 645… ## $ listings &lt;dbl&gt; 701, 746, 784, 785, 794, 780, 742, 765, 771, 764, 721, 658,… ## $ inventory &lt;dbl&gt; 6.3, 6.6, 6.8, 6.9, 6.8, 6.6, 6.2, 6.4, 6.5, 6.6, 6.2, 5.7,… ## $ date &lt;dbl&gt; 2000.000, 2000.083, 2000.167, 2000.250, 2000.333, 2000.417,… Find median and means with summarize(): tx_housing %&gt;% summarize(median(volume), mean(sales), mean(inventory)) ## # A tibble: 1 x 3 ## `median(volume)` `mean(sales)` `mean(inventory)` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 26240116. 603. 7.17 Use ggplot2 to build a bar chart of purpose: ggplot(data=tx_housing, aes(x = city)) + geom_bar() + coord_flip() Use recode() to create the new purpose_recode variable tx_housing$city_recode &lt;- tx_housing$city %&gt;% recode(&quot;Bay Area&quot; = &quot;California&quot;, &quot;El Paso&quot; = &quot;California&quot;) Build a linear regression model, purpose_recode_model: purpose_recode_model &lt;- lm(sales ~ city_recode, data = tx_housing) # Examine results of purpose_recode_model summary(purpose_recode_model) ## ## Call: ## lm(formula = sales ~ city_recode, data = tx_housing) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2938.1 -40.2 -2.5 30.5 3353.9 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 150.462 23.162 6.496 8.80e-11 *** ## city_recodeAmarillo 87.680 32.936 2.662 0.007781 ** ## city_recodeArlington 272.425 32.756 8.317 &lt; 2e-16 *** ## city_recodeAustin 1846.227 32.712 56.438 &lt; 2e-16 *** ## city_recodeBeaumont 26.596 32.712 0.813 0.416221 ## city_recodeBrazoria County -62.400 36.194 -1.724 0.084743 . ## city_recodeBrownsville -92.975 41.873 -2.220 0.026425 * ## city_recodeBryan-College Station 36.281 32.712 1.109 0.267428 ## city_recodeCalifornia 338.886 28.706 11.805 &lt; 2e-16 *** ## city_recodeCollin County 931.871 32.756 28.449 &lt; 2e-16 *** ## city_recodeCorpus Christi 194.427 33.028 5.887 4.12e-09 *** ## city_recodeDallas 4205.000 32.756 128.373 &lt; 2e-16 *** ## city_recodeDenton County 476.242 32.756 14.539 &lt; 2e-16 *** ## city_recodeFort Bend 669.758 32.756 20.447 &lt; 2e-16 *** ## city_recodeFort Worth 622.441 32.756 19.002 &lt; 2e-16 *** ## city_recodeGalveston -65.862 34.666 -1.900 0.057484 . ## city_recodeGarland 42.683 32.756 1.303 0.192600 ## city_recodeHarlingen -85.571 36.987 -2.314 0.020721 * ## city_recodeHouston 5440.672 32.756 166.096 &lt; 2e-16 *** ## city_recodeIrving -30.478 32.756 -0.930 0.352161 ## city_recodeKerrville -106.291 43.005 -2.472 0.013475 * ## city_recodeKilleen-Fort Hood 64.930 33.892 1.916 0.055430 . ## city_recodeLaredo -61.776 41.192 -1.500 0.133732 ## city_recodeLongview-Marshall 35.689 34.995 1.020 0.307840 ## city_recodeLubbock 113.511 32.756 3.465 0.000533 *** ## city_recodeLufkin -103.349 40.871 -2.529 0.011471 * ## city_recodeMcAllen 5.969 38.104 0.157 0.875530 ## city_recodeMidland -4.081 38.559 -0.106 0.915706 ## city_recodeMontgomery County 410.027 32.756 12.518 &lt; 2e-16 *** ## city_recodeNacogdoches -120.412 37.177 -3.239 0.001206 ** ## city_recodeNE Tarrant County 532.387 32.756 16.253 &lt; 2e-16 *** ## city_recodeOdessa -59.912 42.235 -1.419 0.156075 ## city_recodeParis -115.998 33.622 -3.450 0.000564 *** ## city_recodePort Arthur -83.849 32.982 -2.542 0.011034 * ## city_recodeSan Angelo -37.368 33.570 -1.113 0.265688 ## city_recodeSan Antonio 1574.038 32.845 47.923 &lt; 2e-16 *** ## city_recodeSan Marcos -128.040 39.563 -3.236 0.001216 ** ## city_recodeSherman-Denison -46.134 32.756 -1.408 0.159050 ## city_recodeSouth Padre Island -121.366 46.324 -2.620 0.008814 ** ## city_recodeTemple-Belton -19.992 34.477 -0.580 0.562030 ## city_recodeTexarkana -73.142 38.443 -1.903 0.057132 . ## city_recodeTyler 97.971 32.712 2.995 0.002755 ** ## city_recodeVictoria -80.922 32.800 -2.467 0.013645 * ## city_recodeWaco 36.947 36.802 1.004 0.315437 ## city_recodeWichita Falls -13.232 32.712 -0.405 0.685850 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 315.9 on 7081 degrees of freedom ## Multiple R-squared: 0.9269, Adjusted R-squared: 0.9264 ## F-statistic: 2040 on 44 and 7081 DF, p-value: &lt; 2.2e-16 Get anova results and save as purpose_recode_anova: purpose_recode_anova &lt;- anova(purpose_recode_model) # Print purpose_recode_anova purpose_recode_anova ## Analysis of Variance Table ## ## Response: sales ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## city_recode 44 8955309044 203529751 2039.7 &lt; 2.2e-16 *** ## Residuals 7081 706580734 99785 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Examine class of purpose_recode_anova: class(purpose_recode_anova) ## [1] &quot;anova&quot; &quot;data.frame&quot; Use aov() to build purpose_aov: # Analysis of variance purpose_aov &lt;- aov(sales ~ city_recode, data = tx_housing) Conduct Tukey’s HSD test to create tukey_output: tukey_output &lt;- TukeyHSD(purpose_aov, &quot;city_recode&quot;, conf.level = 0.95) # Tidy tukey_output to make sense of the results tidy(tukey_output) ## # A tibble: 990 x 7 ## term contrast null.value estimate conf.low conf.high adj.p.value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 city_re… Amarillo-Abilene 0 87.7 -42.3 218. 8.41e- 1 ## 2 city_re… Arlington-Abilene 0 272. 143. 402. 8.51e-12 ## 3 city_re… Austin-Abilene 0 1846. 1717. 1975. 7.92e-12 ## 4 city_re… Beaumont-Abilene 0 26.6 -102. 156. 1.00e+ 0 ## 5 city_re… Brazoria County-… 0 -62.4 -205. 80.4 1.00e+ 0 ## 6 city_re… Brownsville-Abil… 0 -93.0 -258. 72.2 9.86e- 1 ## 7 city_re… Bryan-College St… 0 36.3 -92.8 165. 1.00e+ 0 ## 8 city_re… California-Abile… 0 339. 226. 452. 7.92e-12 ## 9 city_re… Collin County-Ab… 0 932. 803. 1061. 7.92e-12 ## 10 city_re… Corpus Christi-A… 0 194. 64.1 325. 4.00e- 6 ## # … with 980 more rows Multiple factor experiments: Use aov() to build purpose_emp_aov purpose_emp_aov &lt;- aov(sales ~ city_recode + volume , data = tx_housing) # Print purpose_emp_aov to the console # purpose_emp_aov #Call summary() to see the p-values: summary(purpose_emp_aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## city_recode 44 8.955e+09 203529751 12992 &lt;2e-16 *** ## volume 1 5.957e+08 595663462 38022 &lt;2e-16 *** ## Residuals 7080 1.109e+08 15666 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Model validation Pre-modeling exploratory data analysis Examine the summary of sales summary(tx_housing$sales) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 6 95 187 603 527 8945 Examine sales by volume: tx_housing %&gt;% group_by(volume) %&gt;% summarize(mean = mean(sales), var = var(sales), median = median(sales)) ## # A tibble: 6,855 x 4 ## volume mean var median ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 835000 14 NA 14 ## 2 1018825 14 NA 14 ## 3 1110000 9 NA 9 ## 4 1156999 6 NA 6 ## 5 1165000 18 NA 18 ## 6 1215000 11 NA 11 ## 7 1260000 23 NA 23 ## 8 1305000 16 NA 16 ## 9 1419500 25 NA 25 ## 10 1434950 22 NA 22 ## # … with 6,845 more rows Make a boxplot of sales by volume ggplot(tx_housing, aes(x = volume, y = sales)) + geom_boxplot() ## Warning: Continuous x aesthetic -- did you forget aes(group=...)? Use aov() to create volume_aov plus call summary() to print results volume_aov &lt;- aov(volume ~ sales, data = tx_housing) summary(volume_aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## sales 1 4.535e+20 4.535e+20 180283 &lt;2e-16 *** ## Residuals 7124 1.792e+19 2.515e+15 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Post-modeling validation plots + variance For a 2x2 grid of plots: par(mfrow = c(2, 2)) # Plot grade_aov plot(volume_aov) Bartlett’s test for homogeneity of variance We can test for homogeneity of variances using bartlett.test(), which takes a formula and a dataset as inputs: - bartlett.test(volume ~ sales, data = tx_housing) Conduct the Kruskal-Wallis rank sum test: kruskal.test() to examine whether volume varies by sales when a non-parametric model is employed kruskal.test(volume ~ sales, data = tx_housing) ## ## Kruskal-Wallis rank sum test ## ## data: volume by sales ## Kruskal-Wallis chi-squared = 6877.9, df = 1702, p-value &lt; 2.2e-16 The low p-value indicates that based on this test, we can be confident in our result, which we found across this experiment, that volume varies by sales Sampling [randomized experiments] load data from NHANES dataset https://wwwn.cdc.gov/nchs/nhanes/continuousnhanes/default.aspx?BeginYear=2015 Import the three datasets using read_xpt(): nhanes_demo &lt;- read_xpt(url(&quot;https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/DEMO_I.XPT&quot;)) nhanes_bodymeasures &lt;- read_xpt(url(&quot;https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/BMX_I.XPT&quot;)) nhanes_medical &lt;- read_xpt(url(&quot;https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/MCQ_I.XPT&quot;)) Merge the 3 datasets you just created to create nhanes_combined: nhanes_combined &lt;- list(nhanes_demo, nhanes_medical, nhanes_bodymeasures) %&gt;% Reduce(function(df1, df2) inner_join(df1, df2, by = &quot;SEQN&quot;), .) Fill in the dplyr code: nhanes_combined %&gt;% group_by(MCQ035) %&gt;% summarize(mean = mean(INDHHIN2, na.rm = TRUE)) ## # A tibble: 4 x 2 ## MCQ035 mean ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 9.89 ## 2 2 10.4 ## 3 9 17.8 ## 4 NA 11.7 Fill in the ggplot2 code: nhanes_combined %&gt;% ggplot(aes(as.factor(MCQ035), INDHHIN2)) + geom_boxplot() + labs(x = &quot;Disease type&quot;, y = &quot;Income&quot;) ## Warning: Removed 273 rows containing non-finite values (stat_boxplot). NHANES Data Cleaning Filter to keep only those greater than 16: nhanes_filter &lt;- nhanes_combined %&gt;% filter(RIDAGEYR &gt; 16) Load simputation &amp; impute bmxwt by riagendr: library(simputation) nhanes_final &lt;- simputation::impute_median(nhanes_filter, INDHHIN2 ~ RIDAGEYR) Recode mcq365d with recode() &amp; examine with count(): nhanes_final$mcq365d &lt;- recode(nhanes_final$MCQ035, `1` = 1, `2` = 2, `9` = 2) nhanes_final %&gt;% count(MCQ035) ## # A tibble: 4 x 2 ## MCQ035 n ## &lt;dbl&gt; &lt;int&gt; ## 1 1 522 ## 2 2 369 ## 3 9 15 ## 4 NA 4981 Resampling NHANES data: Use sample_n() to create nhanes_srs: nhanes_srs &lt;- nhanes_final %&gt;% sample_n(2500) Create nhanes_stratified with group_by() and sample_n() nhanes_stratified &lt;- nhanes_final %&gt;% group_by(RIDAGEYR) %&gt;% sample_n(2000, replace = TRUE) nhanes_stratified %&gt;% count(RIDAGEYR) ## # A tibble: 64 x 2 ## # Groups: RIDAGEYR [64] ## RIDAGEYR n ## &lt;dbl&gt; &lt;int&gt; ## 1 17 2000 ## 2 18 2000 ## 3 19 2000 ## 4 20 2000 ## 5 21 2000 ## 6 22 2000 ## 7 23 2000 ## 8 24 2000 ## 9 25 2000 ## 10 26 2000 ## # … with 54 more rows Load sampling package and create nhanes_cluster with cluster(): library(sampling) nhanes_cluster &lt;- cluster(nhanes_final, c(&quot;INDHHIN2&quot;), 6, method = &quot;srswor&quot;) Randomized complete block designs (RCBD): use library(agricolae) block = experimental groups are blocked to be similar (e.g. by sex) complete = each treatment is used the same of times in every block randomized = the treatment is assigned randomly inside each block Create designs using ls(): designs &lt;- ls(&quot;package:agricolae&quot;, pattern = &quot;design&quot;) print(designs) ## [1] &quot;design.ab&quot; &quot;design.alpha&quot; &quot;design.bib&quot; &quot;design.crd&quot; ## [5] &quot;design.cyclic&quot; &quot;design.dau&quot; &quot;design.graeco&quot; &quot;design.lattice&quot; ## [9] &quot;design.lsd&quot; &quot;design.mat&quot; &quot;design.rcbd&quot; &quot;design.split&quot; ## [13] &quot;design.strip&quot; &quot;design.youden&quot; Use str() to view design.rcbd’s criteria: str(design.rcbd) ## function (trt, r, serie = 2, seed = 0, kinds = &quot;Super-Duper&quot;, first = TRUE, ## continue = FALSE, randomization = TRUE) Build treats and rep treats &lt;- LETTERS[1:5] blocks &lt;- 4 blocks ## [1] 4 NHANES RCBD: Build my_design_rcbd and view the sketch my_design_rcbd &lt;- design.rcbd(treats, r = blocks, seed = 42) my_design_rcbd$sketch ## [,1] [,2] [,3] [,4] [,5] ## [1,] &quot;D&quot; &quot;A&quot; &quot;C&quot; &quot;B&quot; &quot;E&quot; ## [2,] &quot;E&quot; &quot;A&quot; &quot;C&quot; &quot;D&quot; &quot;B&quot; ## [3,] &quot;D&quot; &quot;B&quot; &quot;E&quot; &quot;A&quot; &quot;C&quot; ## [4,] &quot;B&quot; &quot;D&quot; &quot;E&quot; &quot;C&quot; &quot;A&quot; Use aov() to create nhanes_rcbd: nhanes_rcbd &lt;- aov(INDHHIN2 ~ MCQ035 + RIDAGEYR, data = nhanes_final) Check results of nhanes_rcbd with summary(): summary(nhanes_rcbd) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## MCQ035 1 1399 1398.5 8.242 0.00419 ** ## RIDAGEYR 1 312 312.4 1.841 0.17519 ## Residuals 903 153221 169.7 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## 4981 observations deleted due to missingness Print mean weights by mcq365d and riagendr: nhanes_final %&gt;% group_by(MCQ035, RIDAGEYR) %&gt;% summarize(mean_ind = mean(INDHHIN2, na.rm = TRUE)) ## # A tibble: 204 x 3 ## # Groups: MCQ035 [4] ## MCQ035 RIDAGEYR mean_ind ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 17 10.3 ## 2 1 18 8 ## 3 1 19 5.38 ## 4 1 20 8 ## 5 1 21 8.75 ## 6 1 22 6.27 ## 7 1 23 7.18 ## 8 1 24 14 ## 9 1 25 9.11 ## 10 1 26 10.6 ## # … with 194 more rows RCBD Model Validation Set up the 2x2 plotting grid and plot nhanes_rcbd par(mfrow = c(2, 2)) plot(nhanes_rcbd) Run the code to view the interaction plots: with(nhanes_final, interaction.plot(MCQ035, RIDAGEYR, INDHHIN2)) Balanced incomplete block design (BIBD) Balanced = each pair of treatment occur together in a block an equal of times Incomplete = not every treatment will appear in every block Use str() to view design.bibd’s criteria str(design.bib) Columns are a blocking factor create my_design_bibd_1 my_design_bibd_1 &lt;- agricolae::design.bib(LETTERS[1:3], k = 3, seed = 42) ## ## Parameters BIB ## ============== ## Lambda : 2 ## treatmeans : 3 ## Block size : 3 ## Blocks : 2 ## Replication: 2 ## ## Efficiency factor 1 ## ## &lt;&lt;&lt; Book &gt;&gt;&gt; create my_design_bibd_2 my_design_bibd_2 &lt;- design.bib(LETTERS[1:8], k = 8, seed = 42) ## ## Parameters BIB ## ============== ## Lambda : 2 ## treatmeans : 8 ## Block size : 8 ## Blocks : 2 ## Replication: 2 ## ## Efficiency factor 1 ## ## &lt;&lt;&lt; Book &gt;&gt;&gt; create my_design_bibd_3: my_design_bibd_3 &lt;- design.bib(LETTERS[1:4], k = 4, seed = 42) ## ## Parameters BIB ## ============== ## Lambda : 2 ## treatmeans : 4 ## Block size : 4 ## Blocks : 2 ## Replication: 2 ## ## Efficiency factor 1 ## ## &lt;&lt;&lt; Book &gt;&gt;&gt; my_design_bibd_3$sketch ## [,1] [,2] [,3] [,4] ## [1,] &quot;C&quot; &quot;A&quot; &quot;D&quot; &quot;B&quot; ## [2,] &quot;C&quot; &quot;D&quot; &quot;B&quot; &quot;A&quot; Build the data.frame: creatinine &lt;- c(1.98, 1.97, 2.35, 2.09, 1.87, 1.95, 2.08, 2.01, 1.84, 2.06, 1.97, 2.22) food &lt;- as.factor(c(&quot;A&quot;, &quot;C&quot;, &quot;D&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;A&quot;, &quot;B&quot;, &quot;D&quot;)) color &lt;- as.factor(rep(c(&quot;Black&quot;, &quot;White&quot;, &quot;Orange&quot;, &quot;Spotted&quot;), each = 3)) cat_experiment &lt;- as.data.frame(cbind(creatinine, food, color)) Create cat_model and examine with summary(): cat_model &lt;- aov(creatinine ~ food + color, data = cat_experiment) summary(cat_model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## food 1 0.01204 0.012042 0.530 0.485 ## color 1 0.00697 0.006971 0.307 0.593 ## Residuals 9 0.20461 0.022735 Calculate lambda, where lamdba is a measure of proportional reduction in error in cross tabulation analysis: DescTools::Lambda(cat_experiment, direction = c(&quot;symmetric&quot;, &quot;row&quot;, &quot;column&quot;), conf.level = NA) ## [1] 0.08636925 Create weightlift_model &amp; examine results: weightlift_model &lt;- aov(MCQ035 ~ INDHHIN2 + RIDAGEYR, data = nhanes_final) summary(weightlift_model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## INDHHIN2 1 9.6 9.614 8.256 0.00416 ** ## RIDAGEYR 1 3.9 3.868 3.321 0.06872 . ## Residuals 903 1051.6 1.165 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## 4981 observations deleted due to missingness Latin Squares Design Key assumption: the treatment and two blocking factors do NOT interact Two blocking factors (instead of one) Analyze like RCBD Mean, var, and median of Math score by Borough: sat_scores &lt;- read.csv(url(&quot;https://data.ct.gov/api/views/kbxi-4ia7/rows.csv?accessType=DOWNLOAD&quot;)) sat_scores %&gt;% group_by(District, Test.takers..2012) %&gt;% summarize(mean = mean(Test.takers..2012, na.rm = TRUE), var = var(Test.takers..2012, na.rm = TRUE), median = median(Test.takers..2012, na.rm = TRUE)) %&gt;% head() ## # A tibble: 6 x 5 ## # Groups: District [6] ## District Test.takers..2012 mean var median ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Amistad Academy District 34 34 NA 34 ## 2 Ansonia 118 118 NA 118 ## 3 Avon 254 254 NA 254 ## 4 Berlin 216 216 NA 216 ## 5 Bethel 200 200 NA 200 ## 6 Bloomfield 14 14 NA 14 Dealing with Missing Test Scores Examine missingness with miss_var_summary() and library(mice): sat_scores %&gt;% miss_var_summary() ## # A tibble: 12 x 3 ## variable n_miss pct_miss ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Test.takers..2013 9 4.57 ## 2 Test.takers..Change. 9 4.57 ## 3 Participation.Rate..estimate...Change. 8 4.06 ## 4 Percent.Meeting.Benchmark..Change. 8 4.06 ## 5 Test.takers..2012 7 3.55 ## 6 Participation.Rate..estimate...2012 7 3.55 ## 7 Participation.Rate..estimate...2013 7 3.55 ## 8 Percent.Meeting.Benchmark..2012 7 3.55 ## 9 Percent.Meeting.Benchmark..2013 7 3.55 ## 10 District.Number 0 0 ## 11 District 0 0 ## 12 School 0 0 sat_scores &lt;- na.omit(sat_scores) mice::md.pattern(sat_scores) ## /\\ /\\ ## { `---&#39; } ## { O O } ## ==&gt; V &lt;== No need for mice. This data set is completely observed. ## \\ \\|/ / ## `-----&#39; ## District.Number District School Test.takers..2012 Test.takers..2013 ## 187 1 1 1 1 1 ## 0 0 0 0 0 ## Test.takers..Change. Participation.Rate..estimate...2012 ## 187 1 1 ## 0 0 ## Participation.Rate..estimate...2013 Participation.Rate..estimate...Change. ## 187 1 1 ## 0 0 ## Percent.Meeting.Benchmark..2012 Percent.Meeting.Benchmark..2013 ## 187 1 1 ## 0 0 ## Percent.Meeting.Benchmark..Change. ## 187 1 0 ## 0 0 Impute the Math score by Borough: sat_scores_2 &lt;- simputation::impute_median(sat_scores, Test.takers..2012 ~ District) #Convert Math score to numeric sat_scores$Average_testtakers2012 &lt;- as.numeric(sat_scores$Test.takers..2012) Examine scores by Borough in both datasets, before and after imputation: sat_scores %&gt;% group_by(District) %&gt;% summarize(median = median(Test.takers..2012, na.rm = TRUE), mean = mean(Test.takers..2012, na.rm = TRUE)) ## # A tibble: 129 x 3 ## District median mean ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Amistad Academy District 34 34 ## 2 Ansonia 118 118 ## 3 Avon 254 254 ## 4 Berlin 216 216 ## 5 Bethel 200 200 ## 6 Bloomfield 65 65 ## 7 Bolton 62 62 ## 8 Branford 196 196 ## 9 Bridgeport 155 202 ## 10 Bristol 211 211 ## # … with 119 more rows sat_scores_2 %&gt;% group_by(District) %&gt;% summarize(median = median(Test.takers..2012), mean = mean(Test.takers..2012)) ## # A tibble: 129 x 3 ## District median mean ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Amistad Academy District 34 34 ## 2 Ansonia 118 118 ## 3 Avon 254 254 ## 4 Berlin 216 216 ## 5 Bethel 200 200 ## 6 Bloomfield 65 65 ## 7 Bolton 62 62 ## 8 Branford 196 196 ## 9 Bridgeport 155 202 ## 10 Bristol 211 211 ## # … with 119 more rows Drawing Latin Squares with agricolae Design a LS with 5 treatments A:E then look at the sketch my_design_lsd &lt;- design.lsd(trt = LETTERS[1:5], seed = 42) my_design_lsd$sketch ## [,1] [,2] [,3] [,4] [,5] ## [1,] &quot;E&quot; &quot;D&quot; &quot;A&quot; &quot;C&quot; &quot;B&quot; ## [2,] &quot;D&quot; &quot;C&quot; &quot;E&quot; &quot;B&quot; &quot;A&quot; ## [3,] &quot;A&quot; &quot;E&quot; &quot;B&quot; &quot;D&quot; &quot;C&quot; ## [4,] &quot;C&quot; &quot;B&quot; &quot;D&quot; &quot;A&quot; &quot;E&quot; ## [5,] &quot;B&quot; &quot;A&quot; &quot;C&quot; &quot;E&quot; &quot;D&quot; Build nyc_scores_ls_lm: sat_scores_ls_lm &lt;- lm(Test.takers..2012 ~ Test.takers..2013 + District, data = sat_scores) # Tidy the results with broom tidy(sat_scores_ls_lm) %&gt;% head() ## # A tibble: 6 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 3.42 23.7 0.144 8.86e- 1 ## 2 Test.takers..2013 0.987 0.0601 16.4 2.85e-23 ## 3 DistrictAnsonia 12.0 33.8 0.355 7.24e- 1 ## 4 DistrictAvon 10.9 35.8 0.303 7.63e- 1 ## 5 DistrictBerlin -4.45 35.3 -0.126 9.00e- 1 ## 6 DistrictBethel 9.14 34.8 0.263 7.94e- 1 Examine the results with anova: anova(sat_scores_ls_lm) ## Analysis of Variance Table ## ## Response: Test.takers..2012 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Test.takers..2013 1 2144936 2144936 3830.0419 &lt;2e-16 *** ## District 128 46850 366 0.6536 0.9749 ## Residuals 57 31922 560 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Graeco-Latin Squares three blocking factors (when there is treatments) Key assumption: the treatment and two blocking factors do NOT interact Analyze like RCBD Drawing Graeco-Latin Squares with agricolae Create trt1 and trt2 Create my_graeco_design trt1 &lt;- LETTERS[1:5] trt2 &lt;- 1:5 my_graeco_design &lt;- design.graeco(trt1, trt2, seed = 42) Examine the parameters and sketch: my_graeco_design$parameters ## $design ## [1] &quot;graeco&quot; ## ## $trt1 ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; ## ## $trt2 ## [1] 1 2 3 4 5 ## ## $r ## [1] 5 ## ## $serie ## [1] 2 ## ## $seed ## [1] 42 ## ## $kinds ## [1] &quot;Super-Duper&quot; ## ## [[8]] ## [1] TRUE my_graeco_design$sketch ## [,1] [,2] [,3] [,4] [,5] ## [1,] &quot;D 5&quot; &quot;A 1&quot; &quot;C 3&quot; &quot;B 4&quot; &quot;E 2&quot; ## [2,] &quot;A 3&quot; &quot;C 4&quot; &quot;B 2&quot; &quot;E 5&quot; &quot;D 1&quot; ## [3,] &quot;C 2&quot; &quot;B 5&quot; &quot;E 1&quot; &quot;D 3&quot; &quot;A 4&quot; ## [4,] &quot;B 1&quot; &quot;E 3&quot; &quot;D 4&quot; &quot;A 2&quot; &quot;C 5&quot; ## [5,] &quot;E 4&quot; &quot;D 2&quot; &quot;A 5&quot; &quot;C 1&quot; &quot;B 3&quot; Create a boxplot of scores by District, with a title and x/y axis labels: ggplot(sat_scores, aes(District, Test.takers..2012)) + geom_boxplot() + labs(title = &quot;Average SAT Math Scores by District in 2012&quot;, x = &quot;District&quot;, y = &quot;Test Takers in 2012&quot;) Build sat_scores_gls_lm: sat_scores_gls_lm &lt;- lm(Test.takers..2012 ~ Test.takers..2013 + District + School, data = sat_scores) # Tidy the results with broom tidy(sat_scores_gls_lm) %&gt;% head() ## # A tibble: 6 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -9.24 NaN NaN NaN ## 2 Test.takers..2013 1.39 NaN NaN NaN ## 3 DistrictAnsonia -17.8 NaN NaN NaN ## 4 DistrictAvon -75.7 NaN NaN NaN ## 5 DistrictBerlin -81.6 NaN NaN NaN ## 6 DistrictBethel -55.8 NaN NaN NaN Examine the results with anova anova(sat_scores_gls_lm) ## Warning in anova.lm(sat_scores_gls_lm): ANOVA F-tests on an essentially perfect ## fit are unreliable ## Analysis of Variance Table ## ## Response: Test.takers..2012 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Test.takers..2013 1 2144936 2144936 ## District 128 46850 366 ## School 57 31922 560 ## Residuals 0 0 Factorial Experiment Design 2 or more factor variables are combined and crossed All of the possible interactions between factors are considered as effect on outcome e.g. high/low water on high/low light Build the boxplot for the district vs. test taker score: ggplot(sat_scores, aes(District, Test.takers..2012)) + geom_boxplot() Create sat_scores_factorial and examine the results: sat_scores_factorial &lt;- aov(Test.takers..2012 ~ Test.takers..2013 * District * School, data = sat_scores) tidy(sat_scores_factorial) %&gt;% head() ## # A tibble: 3 x 4 ## term df sumsq meansq ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Test.takers..2013 1 2144936. 2144936. ## 2 District 128 46850. 366. ## 3 School 57 31922. 560. Evaluating the sat_scores Factorial Model Use shapiro.test() to test the outcome: shapiro.test(sat_scores$Test.takers..2013) ## ## Shapiro-Wilk normality test ## ## data: sat_scores$Test.takers..2013 ## W = 0.91495, p-value = 6.28e-09 "]
]
